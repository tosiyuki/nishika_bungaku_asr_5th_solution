import evaluate
import pandas as pd
import torch
import transformers
import soundfile as sf
import librosa

from datasets import DatasetDict
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Union
from transformers import (
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    WhisperForConditionalGeneration,
    WhisperProcessor,
    WhisperFeatureExtractor,
    WhisperTokenizer,
    set_seed
)

INPUT_PATH = "./data"
MODEL_NAME = "kotoba-tech/kotoba-whisper-v1.0"
SEED = 42
VAL_RATE = 0.0
MODEL_DIR = "./whisper_finetune"

class LazySupervisedDataset(torch.utils.data.Dataset):
    """Dataset for supervised fine-tuning.
    data_dict example:
        {
            audio: ['fine_tune/022CmvRb8aM5pKF_1.mp3', 'fine_tune/022CmvRb8aM5pKF_2.mp3', 'fine_tune/022CmvRb8aM5pKF_3.mp3', 'fine_tune/022CmvRb8aM5pKF_4.mp3', 'fine_tune/022CmvRb8aM5pKF_5.mp3']
            sentence: ['å¹¸ã‚ã‚Šã¦', 'æ™®é€šã®äººãªã‚‰ãŸã„ã—ã¦å•é¡Œã«ã™ã¾ã„ã“ã®ã“ã¨ãŒã€', 'ç§ã®å¿ƒã‚’æš—ãã—ãŸã€‚', 'ã‚‚ã—è€³ãŒã“ã®ã¾ã¾èã“ãˆãªããªã£ãŸã‚‰ã€', 'ãã®æ™‚ã¯è‡ªæ®ºã™ã‚‹ã‚ˆã‚Šã»ã‹ã¯ãªã„ã¨æ€ã£ãŸã€‚']    
        }
    """
    def __init__(
        self, 
        data_dict: dict[str, list[str]],
        tokenizer: transformers.PreTrainedTokenizer,
        feature_extractor,
    ):
        super(LazySupervisedDataset, self).__init__()

        print("Formatting inputs...Skip in lazy mode")
        self.data_dict = data_dict
        self.tokenizer = tokenizer
        self.feature_extractor = feature_extractor

    def __len__(self):
        return len(self.data_dict["audio"])

    def __getitem__(self, i) -> Dict[str, torch.Tensor]:
        audio = self.data_dict["audio"][i]
        sentence = self.data_dict["sentence"][i]

        # audioã®å‰å‡¦ç†
        #audio_array, sampling_rate = sf.read(audio, samplerate=16000)
        audio_array, sampling_rate = librosa.load(audio, sr=16000)
        input_features = feature_extractor(audio_array, sampling_rate=sampling_rate).input_features[0]

        # sentenceã®å‰å‡¦ç†
        labels = self.tokenizer(sentence).input_ids

        data_dict = dict(
            input_features=input_features,
            labels=labels
        )

        return data_dict


@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:

        #éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        # print(f"éŸ³å£°ãƒ‡ãƒ¼ã‚¿ğŸº")
        # print(f"ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‰:{input_features}")
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt") #ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¡Œã„ã€pytorchãƒ†ãƒ³ã‚½ãƒ«(pt)å½¢å¼ã¨ã—ã¦å—ã‘å–ã‚‹
        # print(f"ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å¾Œ:{input_features}")


        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° & filling(ç½®ãæ›ãˆ))å‡¦ç†
        # print(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ğŸ“—")
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        # print(f"ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‰:{label_features}")
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")#ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¡Œã„ã€pytorchãƒ†ãƒ³ã‚½ãƒ«(pt)å½¢å¼

        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)#ä¸é©åˆ‡ãªå€¤ã‚’æ¤œå‡ºã—ã€-100ã§fillã™ã‚‹ï¼ˆç½®ãæ›ãˆã‚‹)
        # print(f"ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å¾Œ:{labels}")


        # if bos token is appended in previous tokenization step,
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels

        return batch


if __name__ == "__main__":
    set_seed(SEED)

    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    train = pd.read_csv(f"{INPUT_PATH}/train.csv")
    train_details = pd.read_csv(f"{INPUT_PATH}/train_details.csv")
    df_train = pd.merge(train, train_details,on="ID")

    # sentenseãŒnanã®è¡Œã¯å‰Šé™¤
    df_train = df_train.dropna(subset=["target_slice"])

    # datasetã®ä½œæˆ
    train_audio = [f"train_vad_rm_noise/{i}.mp3" for i in df_train["DETAIL_ID"].to_list()]
    train_sentence = df_train["target_slice"].tolist()
    n_train = int(len(train_sentence)* (1-VAL_RATE))

    print(f"n_train: {n_train}")

    train_dataset = {
        "audio":train_audio[0:n_train],
        "sentence":train_sentence[0:n_train]
    }

    #validation data
    if VAL_RATE != 0.0:
        val_dataset = {
            "audio":train_audio[n_train:],
            "sentence":train_sentence[n_train:]
        }

    #ç¢ºèª
    print(f"audio sample:{train_dataset['audio'][0:5]}")
    print(f"sentence sample:{train_dataset['sentence'][0:5]}")

    #ãƒ¢ãƒ‡ãƒ«
    model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)
    model.generation_config.language = "ja" #æ—¥æœ¬èª
    model.generation_config.task = "transcribe" #éŸ³å£°èªè­˜ã‚¿ã‚¹ã‚¯ã®ä¸€ç¨®ã§ã‚ã‚‹ã€æ–‡å­—èµ·ã“ã—(transcribe)ã‚’è¡Œã†
    model.generation_config.forced_decoder_ids = None

    #ãƒ—ãƒ­ã‚»ãƒƒã‚µ
    processor = WhisperProcessor.from_pretrained(MODEL_NAME, language="ja", task="transcribe")

    #feature_extractor
    feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME)

    #tokenizer
    tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME, language="ja", task="transcribe")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    novel_dict = DatasetDict()

    novel_dict["train"] = LazySupervisedDataset(train_dataset, tokenizer, feature_extractor)
    if VAL_RATE != 0.0:
        novel_dict["val"] = LazySupervisedDataset(val_dataset, tokenizer, feature_extractor)

     # Data Collatorã‚’å®šç¾©
    data_collator = DataCollatorSpeechSeq2SeqWithPadding(
        processor=processor,
        decoder_start_token_id=model.config.decoder_start_token_id,
    )

    # è©•ä¾¡é–¢æ•°ã®å®šç¾©(CER)
    metric = evaluate.load("cer")

    def compute_metrics(pred):
        pred_ids = pred.predictions #æ–‡å­—èµ·ã“ã—çµæœ
        label_ids = pred.label_ids #æ­£è§£

        # replace -100 with the pad_token_id
        label_ids[label_ids == -100] = tokenizer.pad_token_id


        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True) #æ–‡å­—èµ·ã“ã—çµæœ
        label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True) #æ­£è§£

        cer = metric.compute(predictions=pred_str, references=label_str)

        return {"cer": cer}

    training_args = Seq2SeqTrainingArguments(
        output_dir=MODEL_DIR,  #ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å…ˆ
        per_device_train_batch_size=16,
        gradient_accumulation_steps=8,
        learning_rate=1e-5,#å­¦ç¿’ç‡
        warmup_ratio=0.03,#å­¦ç¿’ç‡ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æœŸé–“
        #max_steps=10,
        num_train_epochs=3,
        seed=SEED,
        gradient_checkpointing=True,
        fp16=True, #GPUã§å­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹
        # fp16=False, #CPUã§å­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹
        evaluation_strategy= "no" if VAL_RATE == 0.0 else "steps",
        per_device_eval_batch_size=32,
        predict_with_generate=True,
        generation_max_length=448,
        save_steps= 500, #æŒ‡å®šã—ãŸã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ã€å­¦ç¿’é€”ä¸­ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹
        eval_steps = None if VAL_RATE == 0.0 else 5, #æŒ‡å®šã—ãŸã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ã€validationãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹
        logging_steps=1, #æŒ‡å®šã—ãŸã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ã€å­¦ç¿’çµŒéã®ãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹
        load_best_model_at_end=VAL_RATE != 0.0,
        metric_for_best_model="cer",
        greater_is_better=False,
        push_to_hub=False, #Falseã«ã™ã‚‹ã¨ã€HuggingFaceã¸ã®ãƒ­ã‚°ã‚¤ãƒ³ç„¡ã—ã§å­¦ç¿’å¯èƒ½
        dataloader_num_workers=16,
        lr_scheduler_type="cosine",
        optim="adamw_torch",
        weight_decay=0.,
    )

    # use_cacheã‚’Falseã«è¨­å®š
    model.config.use_cache = False

    trainer = Seq2SeqTrainer(
        args=training_args, #ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        model=model,#ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹"whisper-small"ãƒ¢ãƒ‡ãƒ«
        train_dataset=novel_dict["train"],
        eval_dataset=None if VAL_RATE == 0.0 else novel_dict["val"],
        data_collator=data_collator, #data_collator (ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†ã‚’è¡Œã†)
        compute_metrics=compute_metrics, #è©•ä¾¡é–¢æ•°
        tokenizer = tokenizer,
    )

    if list(Path(MODEL_DIR).glob("checkpoint-*")):
        trainer.train(resume_from_checkpoint=True)
    else:
        trainer.train()

    trainer.save_model(MODEL_DIR) #ãƒ¢ãƒ‡ãƒ«
    processor.save_pretrained(MODEL_DIR) #ãƒ—ãƒ­ã‚»ãƒƒã‚µ